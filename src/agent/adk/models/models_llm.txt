# DEVELOPER GUIDE: models

## Quick Summary
The `models` directory contains concrete implementations of Large Language Model (LLM) clients that wrap various LLM APIs. These classes provide a standardized interface for interacting with different LLM providers by translating ADK's `LlmRequest` format to provider-specific formats and parsing responses back to standardized `LlmResponse` objects.

## Files Overview
- `lite_llm.py` - LLM client wrapper using the `litellm` library to support hundreds of models from various providers (OpenAI, Anthropic, Vertex AI, etc.)

## Developer API Reference

### lite_llm.py
**Purpose:** Provides the `LiteLlm` class, a `BaseLlm` implementation that interfaces with hundreds of LLM models through the `litellm` library. Supports models from OpenAI, Anthropic, Vertex AI, and many other providers by simply changing the model string.

**Import:** `from google.adk.models.lite_llm import LiteLlm`

**Classes:**
- `LiteLlm(model: str, **kwargs)` - Wrapper around `litellm` for accessing various LLM providers
  - `generate_content_async(llm_request: LlmRequest, stream: bool = False) -> AsyncGenerator[LlmResponse, None]` - Generates content asynchronously with optional streaming
  - `supported_models() -> list[str]` - Returns list of supported models (empty for LiteLlm as it supports dynamic model list)
  - `model: str` - The name of the LiteLlm model
  - `llm_client: LiteLLMClient` - The LLM client instance used for API calls

- `LiteLLMClient()` - Internal client for making API calls (used for testability)
  - `acompletion(model, messages, tools, **kwargs) -> Union[ModelResponse, CustomStreamWrapper]` - Asynchronous completion call
  - `completion(model, messages, tools, stream=False, **kwargs) -> Union[ModelResponse, CustomStreamWrapper]` - Synchronous completion call

- `FunctionChunk(BaseModel)` - Represents a function call chunk in streaming responses
  - `id: Optional[str]` - Function call ID
  - `name: Optional[str]` - Function name
  - `args: Optional[str]` - Function arguments as JSON string
  - `index: Optional[int]` - Index of the function call

- `TextChunk(BaseModel)` - Represents a text chunk in streaming responses
  - `text: str` - The text content

- `UsageMetadataChunk(BaseModel)` - Represents usage metadata in streaming responses
  - `prompt_tokens: int` - Number of prompt tokens
  - `completion_tokens: int` - Number of completion tokens
  - `total_tokens: int` - Total number of tokens

**Functions:**
- `supported_models() -> list[str]` - Static method that returns supported models list

**Usage Examples:**
```python
import asyncio
import os
from google.adk.models.lite_llm import LiteLlm
from google.adk.models.llm_request import LlmRequest, LlmConfig
from google.genai import types

# Set up environment variables for your chosen provider
# For OpenAI:
# os.environ["OPENAI_API_KEY"] = "your-api-key"
# For Vertex AI:
# os.environ["VERTEXAI_PROJECT"] = "your-project-id"
# os.environ["VERTEXAI_LOCATION"] = "your-location"

async def basic_usage():
    # Initialize LiteLlm with a specific model
    llm = LiteLlm(
        model="gpt-4-turbo",
        temperature=0.7,
        max_tokens=150
    )
    
    # Create a request
    request = LlmRequest(
        contents=[
            types.Content(
                role="user",
                parts=[types.Part.from_text("Explain quantum computing in simple terms.")]
            )
        ],
        config=LlmConfig(temperature=0.5)
    )
    
    # Non-streaming response
    async for response in llm.generate_content_async(request, stream=False):
        print(f"Response: {response.text}")
        if response.usage_metadata:
            print(f"Tokens used: {response.usage_metadata.total_token_count}")

async def streaming_usage():
    llm = LiteLlm(model="claude-3-opus-20240229")
    
    request = LlmRequest(
        contents=[
            types.Content(
                role="user", 
                parts=[types.Part.from_text("Write a short story about AI.")]
            )
        ]
    )
    
    # Streaming response
    print("Streaming response:")
    async for response in llm.generate_content_async(request, stream=True):
        if response.text:
            print(response.text, end="", flush=True)
        if response.usage_metadata:
            print(f"\nTotal tokens: {response.usage_metadata.total_token_count}")

async def function_calling_usage():
    # Define a function for the LLM to call
    weather_function = types.FunctionDeclaration(
        name="get_weather",
        description="Get current weather for a location",
        parameters=types.Schema(
            type=types.Type.OBJECT,
            properties={
                "location": types.Schema(
                    type=types.Type.STRING,
                    description="City name"
                )
            },
            required=["location"]
        )
    )
    
    llm = LiteLlm(model="gpt-4-turbo")
    
    request = LlmRequest(
        contents=[
            types.Content(
                role="user",
                parts=[types.Part.from_text("What's the weather like in Tokyo?")]
            )
        ],
        config=LlmConfig(
            tools=[types.Tool(function_declarations=[weather_function])]
        )
    )
    
    async for response in llm.generate_content_async(request):
        if response.function_calls:
            for func_call in response.function_calls:
                print(f"Function called: {func_call.name}")
                print(f"Arguments: {func_call.args}")

async def vertex_ai_usage():
    # Using Vertex AI models through LiteLlm
    os.environ["VERTEXAI_PROJECT"] = "your-gcp-project-id"
    os.environ["VERTEXAI_LOCATION"] = "your-gcp-location"
    
    llm = LiteLlm(model="vertex_ai/claude-3-7-sonnet@20250219")
    
    request = LlmRequest(
        contents=[
            types.Content(
                role="user",
                parts=[types.Part.from_text("Summarize the benefits of cloud computing.")]
            )
        ],
        config=LlmConfig(
            temperature=0.3,
            max_output_tokens=200
        )
    )
    
    async for response in llm.generate_content_async(request):
        print(f"Vertex AI Response: {response.text}")

# Run examples
if __name__ == "__main__":
    # Uncomment to run examples (ensure environment variables are set)
    # asyncio.run(basic_usage())
    # asyncio.run(streaming_usage())
    # asyncio.run(function_calling_usage())
    # asyncio.run(vertex_ai_usage())
    pass
```

# content_hash: 53b50802d5729266fa0ba5df7d713dd050a91a629ab528e7567ed8780b35413e

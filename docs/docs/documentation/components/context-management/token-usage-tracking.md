---
title: Token Usage Tracking
sidebar_position: 2
---

# Token Usage Tracking

Token usage tracking provides visibility into LLM token consumption across your Agent Mesh deployment. You can monitor prompt tokens, completion tokens, cached tokens, and estimated costs through the web UI or API endpoints.

## How Token Tracking Works

When an agent makes an LLM call, the model returns usage metadata that includes the number of tokens consumed. Agent Mesh captures this information at the LiteLLM wrapper level and propagates it through the system to the usage tracking service, which persists the data for later analysis.

The tracking pipeline follows this flow:

1. The LiteLLM model wrapper receives the LLM response with usage metadata
2. The ADK callbacks extract the usage information and pass it to the task execution context
3. The task execution context accumulates usage across multiple LLM calls within a task
4. The usage tracking service persists the aggregated data to the database
5. The frontend retrieves usage data through API endpoints and displays it in the UI

## Enabling Token Usage Tracking

Token usage tracking requires configuration at two levels: the backend model configuration and the frontend gateway configuration.

### Backend Configuration

To enable token usage tracking in the backend, add the `track_token_usage` parameter to your model configuration:

```yaml
# In shared_config.yaml or agent config
llm_config:
  model: claude-sonnet-4-5
  api_base: ${LLM_SERVICE_ENDPOINT}
  api_key: ${LLM_SERVICE_API_KEY}
  track_token_usage: true
```

When `track_token_usage` is set to `true`, the LiteLLM wrapper extracts all usage metadata from LLM responses, including:

- Prompt tokens (tokens sent to the model)
- Completion tokens (tokens generated by the model)
- Total tokens (sum of prompt and completion)
- Cached tokens (tokens served from provider cache, if supported)

When `track_token_usage` is set to `false` or omitted (the default), no usage metadata is reported. This means the frontend will not display any usage information regardless of the frontend configuration.

### Frontend Configuration

To display token usage in the web UI, enable the `tokenUsageTracking` feature flag in your gateway configuration:

```yaml
# In webui.yaml
frontend_feature_enablement:
  tokenUsageTracking: true
```

This flag controls the visibility of usage-related UI components:

- The usage bar in the user menu
- The Usage Details page
- The context usage indicator in chat sessions

If the backend is not tracking usage (because `track_token_usage` is `false`), the frontend components will show no data even when enabled.

## Viewing Usage Data

### Context Usage Indicator

The context usage indicator appears in the chat interface and shows real-time token consumption for the current session. It displays a progress bar indicating how much of the model's context window has been used, along with a breakdown of prompt and completion tokens.

When token usage approaches the model's context limit, the indicator changes color to warn you about potential context exhaustion. At high usage levels, you may want to consider compressing the conversation context to continue the session.

### Usage Details Page

The Usage Details page provides a comprehensive view of token consumption across all sessions. You can access it from the user menu in the web UI. The page displays:

- Total tokens used in the current period
- Breakdown by session
- Cost estimates based on model pricing
- Historical usage trends

### API Endpoints

You can also retrieve usage data programmatically through the API:

- `GET /api/v1/token-usage/session/{session_id}` returns usage for a specific session
- `GET /api/v1/token-usage/current` returns usage for the current user

## Cached Token Tracking

Some LLM providers support prompt caching, which stores frequently used prompt content to reduce latency and costs. When caching is active, the provider may return a `cached_tokens` count indicating how many prompt tokens were served from cache rather than processed fresh.

Agent Mesh extracts cached token information from the `prompt_tokens_details.cached_tokens` field in LLM responses. This information is included in the usage metadata when `track_token_usage` is enabled.

### Provider Support

| Provider | Cached Token Support |
|----------|---------------------|
| Anthropic (Claude) | Yes |
| OpenAI | Yes (with caching enabled) |
| Google (Gemini) | Limited |

## Troubleshooting

### No Usage Data Displayed

If the frontend shows no usage data:

1. Verify that `track_token_usage: true` is set in your model configuration
2. Verify that `tokenUsageTracking: true` is set in `frontend_feature_enablement`
3. Ensure the session service is configured to use SQL persistence (`session_service.type: sql`)

### Cached Tokens Always Show Zero

If cached tokens are always reported as zero:

1. Verify that your LLM provider supports prompt caching
2. Check that your prompts meet the provider's minimum length requirements for caching
3. Enable debug logging to inspect raw LLM responses

## Configuration Reference

### Model Configuration Options

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `track_token_usage` | boolean | `false` | Enable token usage tracking |
| `cache_strategy` | string | `"5m"` | Prompt caching strategy (`"none"`, `"5m"`, `"1h"`) |

### Gateway Configuration Options

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `frontend_feature_enablement.tokenUsageTracking` | boolean | `false` | Show usage UI components |